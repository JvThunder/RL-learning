{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "\n",
    "rewards = 0\n",
    "done = False\n",
    "trunc = False\n",
    "state, info = env.reset()\n",
    "\n",
    "while not done and not trunc:\n",
    "    action = env.action_space.sample()\n",
    "    new_state, reward, done, trunc, info = env.step(action)\n",
    "    if type(reward)==np.ndarray: reward = reward[0]\n",
    "    rewards += reward\n",
    "    env.render()\n",
    "\n",
    "env.close()\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Use GPU 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                320       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,466\n",
      "Trainable params: 2,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Neural network \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "input_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "print(input_dim, n_actions)\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim = input_dim , activation = 'relu'),\n",
    "    Dense(32, activation = 'relu'),\n",
    "    Dense(n_actions, activation = 'linear')\n",
    "])\n",
    "model.compile(optimizer=Adam(), loss = 'mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(replay_memory, model, gamma, minibatch_size=32):\n",
    "    # choose <s,a,r,s',done> experiences randomly from the memory\n",
    "    minibatch = np.random.choice(replay_memory, minibatch_size, replace=True)\n",
    "    # create one list containing s, one list containing a, etc\n",
    "    s_l =      np.array(list(map(lambda x: x['s'], minibatch)))\n",
    "    a_l =      np.array(list(map(lambda x: x['a'], minibatch)))\n",
    "    r_l =      np.array(list(map(lambda x: x['r'], minibatch)))\n",
    "    sprime_l = np.array(list(map(lambda x: x['sprime'], minibatch)))\n",
    "    done_l   = np.array(list(map(lambda x: x['done'], minibatch)))\n",
    "    # Find q(s', a') for all possible actions a'. Store in list\n",
    "    # We'll use the maximum of these values for q-update  \n",
    "    qvals_sprime_l = model.predict(sprime_l, verbose=0)\n",
    "    # Find q(s,a) for all possible actions a. Store in list\n",
    "    target_f = model.predict(s_l, verbose=0)\n",
    "    # q-update target\n",
    "    # For the action we took, use the q-update value  \n",
    "    # For other actions, use the current nnet predicted value\n",
    "    for i,(s,a,r,qvals_sprime, done) in enumerate(zip(s_l,a_l,r_l,qvals_sprime_l, done_l)): \n",
    "        if not done:  target = r + gamma * np.max(qvals_sprime)\n",
    "        else:         target = r\n",
    "        target_f[i][a] = target\n",
    "    # Update weights of neural network with fit() \n",
    "    # Loss function is 0 for actions we didn't take\n",
    "    model.fit(s_l, target_f, epochs=1, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward episode 0 : 12.0\n",
      "Reward episode 1 : 15.0\n",
      "Reward episode 2 : 36.0\n",
      "Reward episode 3 : 42.0\n"
     ]
    }
   ],
   "source": [
    "EPISODES = 150\n",
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "decay_rate = 0.01\n",
    "minibatch_size = 32\n",
    "reward_list = []  # stores rewards of each epsiode \n",
    "replay_memory = [] # replay memory holds s, a, r, s'\n",
    "mem_max_size = 100000\n",
    "\n",
    "reward_list = []\n",
    "for episode in range(EPISODES):\n",
    "    rewards = 0\n",
    "    done = False\n",
    "    trunc = False\n",
    "    state, info = env.reset()\n",
    "\n",
    "    while not done and not trunc:\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            qvals_s = model.predict(state.reshape(1,-1), verbose=0)\n",
    "            action = np.argmax(qvals_s)\n",
    "        \n",
    "        new_state, reward, done, trunc, info = env.step(action)\n",
    "        if type(reward)==np.ndarray: reward = reward[0]\n",
    "        rewards += reward\n",
    "        \n",
    "        if len(replay_memory) > mem_max_size: replay_memory.pop(0)\n",
    "        replay_memory.append({\"s\":state,\n",
    "                              \"a\":action,\n",
    "                              \"r\":reward,\n",
    "                              \"sprime\":new_state,\n",
    "                              \"done\":done})\n",
    "        model = replay(replay_memory, model = model, gamma = gamma, minibatch_size = minibatch_size)\n",
    "        state = new_state\n",
    "    \n",
    "    print(\"Reward episode\",episode,\":\",rewards)\n",
    "    reward_list.append(rewards)\n",
    "    rewards = 0\n",
    "    if epsilon > 0.01: epsilon = np.exp(-decay_rate*episode)\n",
    "\n",
    "print(\"Complete Training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "rewards = 0\n",
    "done = False\n",
    "trunc = False\n",
    "state, info = env.reset()\n",
    "\n",
    "while not done and not trunc:\n",
    "    rewards = 0\n",
    "    done = False\n",
    "    trunc = False\n",
    "    state, info = env.reset(seed=1)\n",
    "\n",
    "    while not done and not trunc:\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            qvals_s = model.predict(state.reshape(1,-1), verbose=0)\n",
    "            action = np.argmax(qvals_s)\n",
    "        \n",
    "        new_state, reward, done, trunc, info = env.step(action)\n",
    "        if type(reward)==np.ndarray: reward = reward[0]\n",
    "        rewards += reward\n",
    "        \n",
    "        model = replay(replay_memory, model = model, gamma = gamma, minibatch_size = minibatch_size)\n",
    "        state = new_state\n",
    "\n",
    "env.close()\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47616051ff63aeeac36103e296d3336537a0c2bffa1b3ec970ae79cf7e9a110c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
