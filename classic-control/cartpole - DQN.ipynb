{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "\n",
    "rewards = 0\n",
    "done = False\n",
    "trunc = False\n",
    "state, info = env.reset(seed=1)\n",
    "\n",
    "while not done and not trunc:\n",
    "    action = env.action_space.sample()\n",
    "    new_state, reward, done, trunc, info = env.step(action)\n",
    "    if type(reward)==np.ndarray: reward = reward[0]\n",
    "    rewards += reward\n",
    "    env.render()\n",
    "\n",
    "env.close()\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Use GPU 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                320       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,466\n",
      "Trainable params: 2,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Neural network \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "input_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "print(input_dim, n_actions)\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim = input_dim , activation = 'relu'),\n",
    "    Dense(32, activation = 'relu'),\n",
    "    Dense(n_actions, activation = 'linear')\n",
    "])\n",
    "model.compile(optimizer=Adam(), loss = 'mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(replay_memory, model, gamma, minibatch_size=32):\n",
    "    # choose <s,a,r,s',done> experiences randomly from the memory\n",
    "    minibatch = np.random.choice(replay_memory, minibatch_size, replace=True)\n",
    "    # create one list containing s, one list containing a, etc\n",
    "    s_l =      np.array(list(map(lambda x: x['s'], minibatch)))\n",
    "    a_l =      np.array(list(map(lambda x: x['a'], minibatch)))\n",
    "    r_l =      np.array(list(map(lambda x: x['r'], minibatch)))\n",
    "    sprime_l = np.array(list(map(lambda x: x['sprime'], minibatch)))\n",
    "    done_l   = np.array(list(map(lambda x: x['done'], minibatch)))\n",
    "    # Find q(s', a') for all possible actions a'. Store in list\n",
    "    # We'll use the maximum of these values for q-update  \n",
    "    qvals_sprime_l = model.predict(sprime_l, verbose=0)\n",
    "    # Find q(s,a) for all possible actions a. Store in list\n",
    "    target_f = model.predict(s_l, verbose=0)\n",
    "    # q-update target\n",
    "    # For the action we took, use the q-update value  \n",
    "    # For other actions, use the current nnet predicted value\n",
    "    for i,(s,a,r,qvals_sprime, done) in enumerate(zip(s_l,a_l,r_l,qvals_sprime_l, done_l)): \n",
    "        if not done:  target = r + gamma * np.max(qvals_sprime)\n",
    "        else:         target = r\n",
    "        target_f[i][a] = target\n",
    "    # Update weights of neural network with fit() \n",
    "    # Loss function is 0 for actions we didn't take\n",
    "    model.fit(s_l, target_f, epochs=1, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward episode 0 : 13.0\n",
      "Reward episode 1 : 38.0\n",
      "Reward episode 2 : 13.0\n",
      "Reward episode 3 : 26.0\n",
      "Reward episode 4 : 34.0\n",
      "Reward episode 5 : 66.0\n",
      "Reward episode 6 : 23.0\n",
      "Reward episode 7 : 15.0\n",
      "Reward episode 8 : 45.0\n",
      "Reward episode 9 : 16.0\n",
      "Reward episode 10 : 17.0\n",
      "Reward episode 11 : 43.0\n",
      "Reward episode 12 : 13.0\n",
      "Reward episode 13 : 13.0\n",
      "Reward episode 14 : 16.0\n",
      "Reward episode 15 : 34.0\n",
      "Reward episode 16 : 42.0\n",
      "Reward episode 17 : 12.0\n",
      "Reward episode 18 : 12.0\n",
      "Reward episode 19 : 14.0\n",
      "Reward episode 20 : 31.0\n",
      "Reward episode 21 : 41.0\n",
      "Reward episode 22 : 15.0\n",
      "Reward episode 23 : 19.0\n",
      "Reward episode 24 : 33.0\n",
      "Reward episode 25 : 15.0\n",
      "Reward episode 26 : 12.0\n",
      "Reward episode 27 : 25.0\n",
      "Reward episode 28 : 18.0\n",
      "Reward episode 29 : 16.0\n",
      "Reward episode 30 : 19.0\n",
      "Reward episode 31 : 43.0\n",
      "Reward episode 32 : 27.0\n",
      "Reward episode 33 : 9.0\n",
      "Reward episode 34 : 22.0\n",
      "Reward episode 35 : 11.0\n",
      "Reward episode 36 : 51.0\n",
      "Reward episode 37 : 39.0\n",
      "Reward episode 38 : 25.0\n",
      "Reward episode 39 : 50.0\n",
      "Reward episode 40 : 23.0\n",
      "Reward episode 41 : 125.0\n",
      "Reward episode 42 : 102.0\n",
      "Reward episode 43 : 19.0\n",
      "Reward episode 44 : 38.0\n",
      "Reward episode 45 : 32.0\n",
      "Reward episode 46 : 106.0\n",
      "Reward episode 47 : 36.0\n",
      "Reward episode 48 : 52.0\n",
      "Reward episode 49 : 104.0\n",
      "Reward episode 50 : 25.0\n",
      "Reward episode 51 : 57.0\n",
      "Reward episode 52 : 57.0\n",
      "Reward episode 53 : 46.0\n",
      "Reward episode 54 : 105.0\n",
      "Reward episode 55 : 85.0\n",
      "Reward episode 56 : 40.0\n",
      "Reward episode 57 : 30.0\n",
      "Reward episode 58 : 30.0\n",
      "Reward episode 59 : 60.0\n",
      "Reward episode 60 : 78.0\n",
      "Reward episode 61 : 131.0\n",
      "Reward episode 62 : 68.0\n",
      "Reward episode 63 : 185.0\n",
      "Reward episode 64 : 16.0\n",
      "Reward episode 65 : 41.0\n",
      "Reward episode 66 : 73.0\n",
      "Reward episode 67 : 13.0\n",
      "Reward episode 68 : 82.0\n",
      "Reward episode 69 : 138.0\n",
      "Reward episode 70 : 122.0\n",
      "Reward episode 71 : 132.0\n",
      "Reward episode 72 : 100.0\n",
      "Reward episode 73 : 32.0\n",
      "Reward episode 74 : 195.0\n",
      "Reward episode 75 : 236.0\n",
      "Reward episode 76 : 312.0\n",
      "Reward episode 77 : 216.0\n",
      "Reward episode 78 : 299.0\n",
      "Reward episode 79 : 23.0\n",
      "Reward episode 80 : 340.0\n",
      "Reward episode 81 : 500.0\n",
      "Reward episode 82 : 218.0\n",
      "Reward episode 83 : 99.0\n",
      "Reward episode 84 : 450.0\n",
      "Reward episode 85 : 257.0\n",
      "Reward episode 86 : 88.0\n",
      "Reward episode 87 : 252.0\n",
      "Reward episode 88 : 303.0\n",
      "Reward episode 89 : 155.0\n",
      "Reward episode 90 : 15.0\n",
      "Reward episode 91 : 156.0\n",
      "Reward episode 92 : 308.0\n",
      "Reward episode 93 : 207.0\n",
      "Reward episode 94 : 224.0\n",
      "Reward episode 95 : 226.0\n",
      "Reward episode 96 : 234.0\n",
      "Reward episode 97 : 400.0\n",
      "Reward episode 98 : 182.0\n",
      "Reward episode 99 : 500.0\n",
      "Reward episode 100 : 256.0\n",
      "Reward episode 101 : 313.0\n",
      "Reward episode 102 : 282.0\n",
      "Reward episode 103 : 319.0\n",
      "Reward episode 104 : 324.0\n",
      "Reward episode 105 : 321.0\n",
      "Reward episode 106 : 353.0\n",
      "Reward episode 107 : 283.0\n",
      "Reward episode 108 : 500.0\n",
      "Reward episode 109 : 357.0\n",
      "Reward episode 110 : 287.0\n",
      "Reward episode 111 : 352.0\n",
      "Reward episode 112 : 77.0\n",
      "Reward episode 113 : 329.0\n",
      "Reward episode 114 : 201.0\n",
      "Reward episode 115 : 83.0\n",
      "Reward episode 116 : 351.0\n",
      "Reward episode 117 : 392.0\n",
      "Reward episode 118 : 500.0\n",
      "Reward episode 119 : 343.0\n",
      "Reward episode 120 : 436.0\n",
      "Reward episode 121 : 313.0\n",
      "Reward episode 122 : 310.0\n",
      "Reward episode 123 : 308.0\n",
      "Reward episode 124 : 474.0\n",
      "Reward episode 125 : 454.0\n",
      "Reward episode 126 : 343.0\n",
      "Reward episode 127 : 425.0\n",
      "Reward episode 128 : 346.0\n",
      "Reward episode 129 : 432.0\n",
      "Reward episode 130 : 344.0\n",
      "Reward episode 131 : 325.0\n",
      "Reward episode 132 : 361.0\n",
      "Reward episode 133 : 313.0\n",
      "Reward episode 134 : 332.0\n",
      "Reward episode 135 : 306.0\n",
      "Reward episode 136 : 262.0\n",
      "Reward episode 137 : 426.0\n",
      "Reward episode 138 : 500.0\n",
      "Reward episode 139 : 500.0\n",
      "Reward episode 140 : 367.0\n",
      "Reward episode 141 : 400.0\n",
      "Reward episode 142 : 280.0\n",
      "Reward episode 143 : 500.0\n",
      "Reward episode 144 : 411.0\n",
      "Reward episode 145 : 492.0\n",
      "Reward episode 146 : 455.0\n",
      "Reward episode 147 : 295.0\n",
      "Reward episode 148 : 361.0\n",
      "Reward episode 149 : 496.0\n",
      "Complete Training!\n"
     ]
    }
   ],
   "source": [
    "EPISODES = 150\n",
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "decay_rate = 0.01\n",
    "minibatch_size = 32\n",
    "reward_list = []  # stores rewards of each epsiode \n",
    "replay_memory = [] # replay memory holds s, a, r, s'\n",
    "mem_max_size = 100000\n",
    "\n",
    "reward_list = []\n",
    "for episode in range(EPISODES):\n",
    "    rewards = 0\n",
    "    done = False\n",
    "    trunc = False\n",
    "    state, info = env.reset(seed=1)\n",
    "\n",
    "    while not done and not trunc:\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            qvals_s = model.predict(state.reshape(1,-1), verbose=0)\n",
    "            action = np.argmax(qvals_s)\n",
    "        \n",
    "        new_state, reward, done, trunc, info = env.step(action)\n",
    "        if type(reward)==np.ndarray: reward = reward[0]\n",
    "        rewards += reward\n",
    "        \n",
    "        if len(replay_memory) > mem_max_size: replay_memory.pop(0)\n",
    "        replay_memory.append({\"s\":state,\n",
    "                              \"a\":action,\n",
    "                              \"r\":reward,\n",
    "                              \"sprime\":new_state,\n",
    "                              \"done\":done})\n",
    "        model = replay(replay_memory, model = model, gamma = gamma, minibatch_size = minibatch_size)\n",
    "        state = new_state\n",
    "    \n",
    "    print(\"Reward episode\",episode,\":\",rewards)\n",
    "    reward_list.append(rewards)\n",
    "    rewards = 0\n",
    "    if epsilon > 0.01: epsilon = np.exp(-decay_rate*episode)\n",
    "\n",
    "print(\"Complete Training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "rewards = 0\n",
    "done = False\n",
    "trunc = False\n",
    "state, info = env.reset(seed=1)\n",
    "\n",
    "while not done and not trunc:\n",
    "    rewards = 0\n",
    "    done = False\n",
    "    trunc = False\n",
    "    state, info = env.reset(seed=1)\n",
    "\n",
    "    while not done and not trunc:\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            qvals_s = model.predict(state.reshape(1,-1), verbose=0)\n",
    "            action = np.argmax(qvals_s)\n",
    "        \n",
    "        new_state, reward, done, trunc, info = env.step(action)\n",
    "        if type(reward)==np.ndarray: reward = reward[0]\n",
    "        rewards += reward\n",
    "        \n",
    "        model = replay(replay_memory, model = model, gamma = gamma, minibatch_size = minibatch_size)\n",
    "        state = new_state\n",
    "\n",
    "env.close()\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47616051ff63aeeac36103e296d3336537a0c2bffa1b3ec970ae79cf7e9a110c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
