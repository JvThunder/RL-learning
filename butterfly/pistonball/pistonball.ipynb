{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "# env = pistonball_v6.parallel_env(render_mode=\"human\")\n",
    "# observations, infos = env.reset()\n",
    "\n",
    "# while env.agents:\n",
    "#     actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "#     observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "#     env.render()\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supersuit as ss\n",
    "    \n",
    "def make_env():\n",
    "    env = pistonball_v6.parallel_env(n_pistons=5, \n",
    "                                    time_penalty=-0.1, \n",
    "                                    continuous=False, \n",
    "                                    random_drop=True, \n",
    "                                    random_rotate=False, \n",
    "                                    ball_mass=0.75, \n",
    "                                    ball_friction=0.3, \n",
    "                                    ball_elasticity=1.5, \n",
    "                                    max_cycles=100)\n",
    "    env = ss.color_reduction_v0(env, mode='B')\n",
    "    env = ss.resize_v1(env, x_size=240, y_size=60)\n",
    "    env = ss.frame_stack_v1(env, 4)\n",
    "    env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "    env = ss.concat_vec_envs_v1(env, 8, num_cpus=4, base_class='stable_baselines3')\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Conda\\envs\\gpu-env\\lib\\site-packages\\stable_baselines3\\common\\callbacks.py:403: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x00000223001F70A0> != <supersuit.vector.sb3_vector_wrapper.SB3VecEnvWrapper object at 0x00000222E33B3E20>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n",
      "d:\\Conda\\envs\\gpu-env\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10240, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -10      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 577   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 17    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20480, episode_reward=-21.24 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 100           |\n",
      "|    mean_reward          | -21.2         |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 20480         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00074759184 |\n",
      "|    clip_fraction        | 0.0174        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | -3.48e-05     |\n",
      "|    learning_rate        | 0.0006        |\n",
      "|    loss                 | 15.2          |\n",
      "|    n_updates            | 5             |\n",
      "|    policy_gradient_loss | 0.00143       |\n",
      "|    value_loss           | 349           |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 571   |\n",
      "|    iterations      | 2     |\n",
      "|    time_elapsed    | 35    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30720, episode_reward=38.05 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | 38.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045587095 |\n",
      "|    clip_fraction        | 0.0402       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -0.0306      |\n",
      "|    learning_rate        | 0.0006       |\n",
      "|    loss                 | 11.9         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | 0.000738     |\n",
      "|    value_loss           | 353          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 566   |\n",
      "|    iterations      | 3     |\n",
      "|    time_elapsed    | 54    |\n",
      "|    total_timesteps | 30720 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40960, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -10          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054545426 |\n",
      "|    clip_fraction        | 0.0482       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | -0.0732      |\n",
      "|    learning_rate        | 0.0006       |\n",
      "|    loss                 | 6.7          |\n",
      "|    n_updates            | 15           |\n",
      "|    policy_gradient_loss | -0.000672    |\n",
      "|    value_loss           | 172          |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 576   |\n",
      "|    iterations      | 4     |\n",
      "|    time_elapsed    | 71    |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n",
      "Eval num_timesteps=51200, episode_reward=99.50 +/- 0.00\n",
      "Episode length: 6.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 6           |\n",
      "|    mean_reward          | 99.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007983419 |\n",
      "|    clip_fraction        | 0.0911      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -0.00264    |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 17.4        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.00104     |\n",
      "|    value_loss           | 421         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 601   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 85    |\n",
      "|    total_timesteps | 51200 |\n",
      "------------------------------\n",
      "Eval num_timesteps=61440, episode_reward=39.35 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | 39.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 61440      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01081794 |\n",
      "|    clip_fraction        | 0.121      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.09      |\n",
      "|    explained_variance   | -0.0464    |\n",
      "|    learning_rate        | 0.0006     |\n",
      "|    loss                 | 20.3       |\n",
      "|    n_updates            | 25         |\n",
      "|    policy_gradient_loss | 0.00194    |\n",
      "|    value_loss           | 395        |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 605   |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 101   |\n",
      "|    total_timesteps | 61440 |\n",
      "------------------------------\n",
      "Eval num_timesteps=71680, episode_reward=61.30 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | 61.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016716488 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | -0.236      |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 7.85        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.00116     |\n",
      "|    value_loss           | 241         |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 607   |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 118   |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "Eval num_timesteps=81920, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -10         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016421711 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | -0.0369     |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 8.1         |\n",
      "|    n_updates            | 35          |\n",
      "|    policy_gradient_loss | 0.00275     |\n",
      "|    value_loss           | 263         |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 610   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 134   |\n",
      "|    total_timesteps | 81920 |\n",
      "------------------------------\n",
      "Eval num_timesteps=92160, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -10        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 92160      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01502468 |\n",
      "|    clip_fraction        | 0.244      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.07      |\n",
      "|    explained_variance   | 0.0116     |\n",
      "|    learning_rate        | 0.0006     |\n",
      "|    loss                 | 7.92       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | 0.0057     |\n",
      "|    value_loss           | 257        |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 612   |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 150   |\n",
      "|    total_timesteps | 92160 |\n",
      "------------------------------\n",
      "Eval num_timesteps=102400, episode_reward=53.10 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | 53.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015633263 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | -0.125      |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 8.79        |\n",
      "|    n_updates            | 45          |\n",
      "|    policy_gradient_loss | 0.00627     |\n",
      "|    value_loss           | 294         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 612    |\n",
      "|    iterations      | 10     |\n",
      "|    time_elapsed    | 167    |\n",
      "|    total_timesteps | 102400 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=112640, episode_reward=1.58 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | 1.58        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 112640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021406624 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | -0.205      |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 9.42        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | 0.0088      |\n",
      "|    value_loss           | 269         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 615    |\n",
      "|    iterations      | 11     |\n",
      "|    time_elapsed    | 182    |\n",
      "|    total_timesteps | 112640 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=122880, episode_reward=99.60 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 99.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 122880     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01681034 |\n",
      "|    clip_fraction        | 0.247      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.02      |\n",
      "|    explained_variance   | -0.255     |\n",
      "|    learning_rate        | 0.0006     |\n",
      "|    loss                 | 9.56       |\n",
      "|    n_updates            | 55         |\n",
      "|    policy_gradient_loss | 0.00204    |\n",
      "|    value_loss           | 228        |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 623    |\n",
      "|    iterations      | 12     |\n",
      "|    time_elapsed    | 197    |\n",
      "|    total_timesteps | 122880 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=133120, episode_reward=21.31 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | 21.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 133120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018299416 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | -0.0419     |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 11.8        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | 0.00445     |\n",
      "|    value_loss           | 334         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 623    |\n",
      "|    iterations      | 13     |\n",
      "|    time_elapsed    | 213    |\n",
      "|    total_timesteps | 133120 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=143360, episode_reward=-0.28 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -0.278      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 143360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018314917 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.964      |\n",
      "|    explained_variance   | -0.192      |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 6.88        |\n",
      "|    n_updates            | 65          |\n",
      "|    policy_gradient_loss | 0.00562     |\n",
      "|    value_loss           | 159         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 623    |\n",
      "|    iterations      | 14     |\n",
      "|    time_elapsed    | 230    |\n",
      "|    total_timesteps | 143360 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=153600, episode_reward=2.09 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | 2.09        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 153600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026754728 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.961      |\n",
      "|    explained_variance   | -0.0871     |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 5.41        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | 0.00808     |\n",
      "|    value_loss           | 124         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 621    |\n",
      "|    iterations      | 15     |\n",
      "|    time_elapsed    | 247    |\n",
      "|    total_timesteps | 153600 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=163840, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -10         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017926788 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.976      |\n",
      "|    explained_variance   | -0.117      |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 7.6         |\n",
      "|    n_updates            | 75          |\n",
      "|    policy_gradient_loss | 0.00325     |\n",
      "|    value_loss           | 201         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 621    |\n",
      "|    iterations      | 16     |\n",
      "|    time_elapsed    | 263    |\n",
      "|    total_timesteps | 163840 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=174080, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -10         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 174080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026196066 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | -0.24       |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 5.38        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | 0.00414     |\n",
      "|    value_loss           | 120         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 622    |\n",
      "|    iterations      | 17     |\n",
      "|    time_elapsed    | 279    |\n",
      "|    total_timesteps | 174080 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=184320, episode_reward=23.05 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | 23.1        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 184320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022341248 |\n",
      "|    clip_fraction        | 0.278       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | -0.146      |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 5.26        |\n",
      "|    n_updates            | 85          |\n",
      "|    policy_gradient_loss | 0.00795     |\n",
      "|    value_loss           | 124         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 623    |\n",
      "|    iterations      | 18     |\n",
      "|    time_elapsed    | 295    |\n",
      "|    total_timesteps | 184320 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=194560, episode_reward=43.66 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | 43.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024650244 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | -0.507      |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 6.43        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.00586     |\n",
      "|    value_loss           | 227         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 623    |\n",
      "|    iterations      | 19     |\n",
      "|    time_elapsed    | 311    |\n",
      "|    total_timesteps | 194560 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=204800, episode_reward=-12.86 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -12.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 204800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019779636 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | -0.0593     |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 7.86        |\n",
      "|    n_updates            | 95          |\n",
      "|    policy_gradient_loss | 0.0105      |\n",
      "|    value_loss           | 249         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 623    |\n",
      "|    iterations      | 20     |\n",
      "|    time_elapsed    | 328    |\n",
      "|    total_timesteps | 204800 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=215040, episode_reward=-4.12 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -4.12       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 215040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023716811 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | -0.132      |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 8.34        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | 0.00989     |\n",
      "|    value_loss           | 222         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 624    |\n",
      "|    iterations      | 21     |\n",
      "|    time_elapsed    | 344    |\n",
      "|    total_timesteps | 215040 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=225280, episode_reward=14.51 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | 14.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 225280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026726633 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | -0.101      |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 10.6        |\n",
      "|    n_updates            | 105         |\n",
      "|    policy_gradient_loss | 0.013       |\n",
      "|    value_loss           | 280         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 625    |\n",
      "|    iterations      | 22     |\n",
      "|    time_elapsed    | 360    |\n",
      "|    total_timesteps | 225280 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=235520, episode_reward=99.30 +/- 0.00\n",
      "Episode length: 8.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 8           |\n",
      "|    mean_reward          | 99.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 235520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024508556 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | -0.153      |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 7.79        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | 0.0133      |\n",
      "|    value_loss           | 218         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 629    |\n",
      "|    iterations      | 23     |\n",
      "|    time_elapsed    | 373    |\n",
      "|    total_timesteps | 235520 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=245760, episode_reward=-35.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -35         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 245760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022388553 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | -0.158      |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 10.5        |\n",
      "|    n_updates            | 115         |\n",
      "|    policy_gradient_loss | 0.00819     |\n",
      "|    value_loss           | 232         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 630    |\n",
      "|    iterations      | 24     |\n",
      "|    time_elapsed    | 389    |\n",
      "|    total_timesteps | 245760 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -10        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 256000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03342383 |\n",
      "|    clip_fraction        | 0.322      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.01      |\n",
      "|    explained_variance   | -0.0854    |\n",
      "|    learning_rate        | 0.0006     |\n",
      "|    loss                 | 5.43       |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | 0.00855    |\n",
      "|    value_loss           | 185        |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 630    |\n",
      "|    iterations      | 25     |\n",
      "|    time_elapsed    | 406    |\n",
      "|    total_timesteps | 256000 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=266240, episode_reward=98.20 +/- 0.00\n",
      "Episode length: 19.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 19          |\n",
      "|    mean_reward          | 98.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 266240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019200515 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.991      |\n",
      "|    explained_variance   | -0.159      |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 8.34        |\n",
      "|    n_updates            | 125         |\n",
      "|    policy_gradient_loss | 0.0129      |\n",
      "|    value_loss           | 259         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 633    |\n",
      "|    iterations      | 26     |\n",
      "|    time_elapsed    | 420    |\n",
      "|    total_timesteps | 266240 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=276480, episode_reward=-27.65 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -27.6      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 276480     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04289682 |\n",
      "|    clip_fraction        | 0.348      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.992     |\n",
      "|    explained_variance   | -0.151     |\n",
      "|    learning_rate        | 0.0006     |\n",
      "|    loss                 | 14.7       |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | 0.0146     |\n",
      "|    value_loss           | 384        |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 633    |\n",
      "|    iterations      | 27     |\n",
      "|    time_elapsed    | 436    |\n",
      "|    total_timesteps | 276480 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=286720, episode_reward=35.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | 35          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 286720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034482457 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | -0.169      |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 9.64        |\n",
      "|    n_updates            | 135         |\n",
      "|    policy_gradient_loss | 0.012       |\n",
      "|    value_loss           | 271         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 632    |\n",
      "|    iterations      | 28     |\n",
      "|    time_elapsed    | 453    |\n",
      "|    total_timesteps | 286720 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=296960, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -10         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 296960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023737255 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | -0.222      |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 8.76        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | 0.0128      |\n",
      "|    value_loss           | 188         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 632    |\n",
      "|    iterations      | 29     |\n",
      "|    time_elapsed    | 469    |\n",
      "|    total_timesteps | 296960 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=307200, episode_reward=-17.53 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -17.5      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 307200     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03197766 |\n",
      "|    clip_fraction        | 0.293      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.987     |\n",
      "|    explained_variance   | 0.0105     |\n",
      "|    learning_rate        | 0.0006     |\n",
      "|    loss                 | 13.9       |\n",
      "|    n_updates            | 145        |\n",
      "|    policy_gradient_loss | 0.00985    |\n",
      "|    value_loss           | 311        |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 632    |\n",
      "|    iterations      | 30     |\n",
      "|    time_elapsed    | 485    |\n",
      "|    total_timesteps | 307200 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=317440, episode_reward=32.03 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | 32         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 317440     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03130228 |\n",
      "|    clip_fraction        | 0.312      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.982     |\n",
      "|    explained_variance   | -0.543     |\n",
      "|    learning_rate        | 0.0006     |\n",
      "|    loss                 | 8.25       |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | 0.0135     |\n",
      "|    value_loss           | 244        |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 632    |\n",
      "|    iterations      | 31     |\n",
      "|    time_elapsed    | 502    |\n",
      "|    total_timesteps | 317440 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=327680, episode_reward=-39.87 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -39.9       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 327680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021426737 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.973      |\n",
      "|    explained_variance   | -0.0762     |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 9.06        |\n",
      "|    n_updates            | 155         |\n",
      "|    policy_gradient_loss | 0.0117      |\n",
      "|    value_loss           | 233         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 632    |\n",
      "|    iterations      | 32     |\n",
      "|    time_elapsed    | 518    |\n",
      "|    total_timesteps | 327680 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=337920, episode_reward=68.65 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | 68.7        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 337920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030869197 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.99       |\n",
      "|    explained_variance   | -0.2        |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 6.81        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | 0.011       |\n",
      "|    value_loss           | 199         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 632    |\n",
      "|    iterations      | 33     |\n",
      "|    time_elapsed    | 534    |\n",
      "|    total_timesteps | 337920 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=348160, episode_reward=-10.85 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -10.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 348160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030889522 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.961      |\n",
      "|    explained_variance   | -0.32       |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 9           |\n",
      "|    n_updates            | 165         |\n",
      "|    policy_gradient_loss | 0.0171      |\n",
      "|    value_loss           | 289         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 632    |\n",
      "|    iterations      | 34     |\n",
      "|    time_elapsed    | 550    |\n",
      "|    total_timesteps | 348160 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=358400, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -10         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 358400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033905275 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.974      |\n",
      "|    explained_variance   | -0.0521     |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 13.4        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | 0.0114      |\n",
      "|    value_loss           | 314         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 632    |\n",
      "|    iterations      | 35     |\n",
      "|    time_elapsed    | 566    |\n",
      "|    total_timesteps | 358400 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=368640, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -10         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 368640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028932387 |\n",
      "|    clip_fraction        | 0.296       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.958      |\n",
      "|    explained_variance   | -0.0969     |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 15.7        |\n",
      "|    n_updates            | 175         |\n",
      "|    policy_gradient_loss | 0.0126      |\n",
      "|    value_loss           | 398         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 632    |\n",
      "|    iterations      | 36     |\n",
      "|    time_elapsed    | 583    |\n",
      "|    total_timesteps | 368640 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=378880, episode_reward=98.10 +/- 0.00\n",
      "Episode length: 20.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 20         |\n",
      "|    mean_reward          | 98.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 378880     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03395199 |\n",
      "|    clip_fraction        | 0.324      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.938     |\n",
      "|    explained_variance   | -0.407     |\n",
      "|    learning_rate        | 0.0006     |\n",
      "|    loss                 | 15.3       |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | 0.0129     |\n",
      "|    value_loss           | 454        |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 634    |\n",
      "|    iterations      | 37     |\n",
      "|    time_elapsed    | 597    |\n",
      "|    total_timesteps | 378880 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=389120, episode_reward=25.48 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | 25.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 389120     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02593577 |\n",
      "|    clip_fraction        | 0.293      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.955     |\n",
      "|    explained_variance   | -0.117     |\n",
      "|    learning_rate        | 0.0006     |\n",
      "|    loss                 | 13.9       |\n",
      "|    n_updates            | 185        |\n",
      "|    policy_gradient_loss | 0.0117     |\n",
      "|    value_loss           | 366        |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 634    |\n",
      "|    iterations      | 38     |\n",
      "|    time_elapsed    | 613    |\n",
      "|    total_timesteps | 389120 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=399360, episode_reward=-40.77 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -40.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 399360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034554474 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.952      |\n",
      "|    explained_variance   | -0.207      |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 11.1        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | 0.0136      |\n",
      "|    value_loss           | 408         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 634    |\n",
      "|    iterations      | 39     |\n",
      "|    time_elapsed    | 629    |\n",
      "|    total_timesteps | 399360 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=409600, episode_reward=-56.91 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -56.9      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 409600     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02687933 |\n",
      "|    clip_fraction        | 0.307      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.966     |\n",
      "|    explained_variance   | -0.134     |\n",
      "|    learning_rate        | 0.0006     |\n",
      "|    loss                 | 13.6       |\n",
      "|    n_updates            | 195        |\n",
      "|    policy_gradient_loss | 0.0108     |\n",
      "|    value_loss           | 394        |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 634    |\n",
      "|    iterations      | 40     |\n",
      "|    time_elapsed    | 645    |\n",
      "|    total_timesteps | 409600 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=419840, episode_reward=-0.91 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -0.909     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 419840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03779677 |\n",
      "|    clip_fraction        | 0.351      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.969     |\n",
      "|    explained_variance   | -0.264     |\n",
      "|    learning_rate        | 0.0006     |\n",
      "|    loss                 | 10.3       |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | 0.0134     |\n",
      "|    value_loss           | 262        |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 633    |\n",
      "|    iterations      | 41     |\n",
      "|    time_elapsed    | 662    |\n",
      "|    total_timesteps | 419840 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=430080, episode_reward=-11.09 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -11.1       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 430080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040762432 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.944      |\n",
      "|    explained_variance   | -0.395      |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 9.99        |\n",
      "|    n_updates            | 205         |\n",
      "|    policy_gradient_loss | 0.0137      |\n",
      "|    value_loss           | 318         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 633    |\n",
      "|    iterations      | 42     |\n",
      "|    time_elapsed    | 679    |\n",
      "|    total_timesteps | 430080 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=440320, episode_reward=-4.44 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | -4.44      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 440320     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03929342 |\n",
      "|    clip_fraction        | 0.325      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.903     |\n",
      "|    explained_variance   | -0.0738    |\n",
      "|    learning_rate        | 0.0006     |\n",
      "|    loss                 | 13.3       |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | 0.0177     |\n",
      "|    value_loss           | 320        |\n",
      "----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 633    |\n",
      "|    iterations      | 43     |\n",
      "|    time_elapsed    | 695    |\n",
      "|    total_timesteps | 440320 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=450560, episode_reward=-58.75 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -58.8       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 450560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035383217 |\n",
      "|    clip_fraction        | 0.35        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.909      |\n",
      "|    explained_variance   | -0.2        |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 9.7         |\n",
      "|    n_updates            | 215         |\n",
      "|    policy_gradient_loss | 0.0138      |\n",
      "|    value_loss           | 254         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 632    |\n",
      "|    iterations      | 44     |\n",
      "|    time_elapsed    | 711    |\n",
      "|    total_timesteps | 450560 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=460800, episode_reward=-105.08 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -105        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 460800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032155275 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.925      |\n",
      "|    explained_variance   | -0.441      |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 6.4         |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | 0.0158      |\n",
      "|    value_loss           | 197         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 632    |\n",
      "|    iterations      | 45     |\n",
      "|    time_elapsed    | 728    |\n",
      "|    total_timesteps | 460800 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=471040, episode_reward=-6.15 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -6.15       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 471040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053212486 |\n",
      "|    clip_fraction        | 0.436       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.941      |\n",
      "|    explained_variance   | -0.446      |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 7.35        |\n",
      "|    n_updates            | 225         |\n",
      "|    policy_gradient_loss | 0.0283      |\n",
      "|    value_loss           | 149         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 631    |\n",
      "|    iterations      | 46     |\n",
      "|    time_elapsed    | 745    |\n",
      "|    total_timesteps | 471040 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=481280, episode_reward=99.40 +/- 0.00\n",
      "Episode length: 7.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 7           |\n",
      "|    mean_reward          | 99.4        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 481280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033144366 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.952      |\n",
      "|    explained_variance   | -0.213      |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 6.63        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | 0.0113      |\n",
      "|    value_loss           | 232         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 633    |\n",
      "|    iterations      | 47     |\n",
      "|    time_elapsed    | 759    |\n",
      "|    total_timesteps | 481280 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=491520, episode_reward=97.01 +/- 0.00\n",
      "Episode length: 12.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 12          |\n",
      "|    mean_reward          | 97          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 491520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035885885 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.921      |\n",
      "|    explained_variance   | -0.274      |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 14          |\n",
      "|    n_updates            | 235         |\n",
      "|    policy_gradient_loss | 0.0127      |\n",
      "|    value_loss           | 328         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 633    |\n",
      "|    iterations      | 48     |\n",
      "|    time_elapsed    | 775    |\n",
      "|    total_timesteps | 491520 |\n",
      "-------------------------------\n",
      "Eval num_timesteps=501760, episode_reward=-10.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -10         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 501760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036591314 |\n",
      "|    clip_fraction        | 0.327       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.924      |\n",
      "|    explained_variance   | -0.0889     |\n",
      "|    learning_rate        | 0.0006      |\n",
      "|    loss                 | 7.86        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | 0.0113      |\n",
      "|    value_loss           | 214         |\n",
      "-----------------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 633    |\n",
      "|    iterations      | 49     |\n",
      "|    time_elapsed    | 791    |\n",
      "|    total_timesteps | 501760 |\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import CnnPolicy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "# Create the evaluation callback\n",
    "eval_callback = EvalCallback(env, best_model_save_path='./best_model/',\n",
    "                             log_path='./logs/', eval_freq=256,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "model = PPO(CnnPolicy, env, verbose=3, \n",
    "            gamma=0.95, \n",
    "            n_steps=256, \n",
    "            ent_coef=0.09, \n",
    "            learning_rate=0.0006, \n",
    "            vf_coef=0.04, \n",
    "            max_grad_norm=0.9, \n",
    "            gae_lambda=0.99, \n",
    "            n_epochs=5, \n",
    "            clip_range=0.2, \n",
    "            batch_size=256)\n",
    "\n",
    "# Pass the evaluation callback to the learn method\n",
    "model.learn(total_timesteps=500_000, callback=eval_callback)\n",
    "model.save(\"policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "model = PPO.load(\"policy\")\n",
    "\n",
    "# Create the environment\n",
    "env = pistonball_v6.env(render_mode='rgb_array', n_pistons=5, \n",
    "                                                time_penalty=-0.1, \n",
    "                                                continuous=False, \n",
    "                                                random_drop=True, \n",
    "                                                random_rotate=False, \n",
    "                                                ball_mass=0.75, \n",
    "                                                ball_friction=0.3, \n",
    "                                                ball_elasticity=1.5, \n",
    "                                                max_cycles=100)\n",
    "env = ss.color_reduction_v0(env, mode='B')\n",
    "env = ss.resize_v1(env, x_size=240, y_size=60)\n",
    "env = ss.frame_stack_v1(env, 4)\n",
    "\n",
    "# Reset the environment\n",
    "obs = env.reset()\n",
    "\n",
    "# Create a list to store the frames of the GIF\n",
    "frames = []\n",
    "done = False\n",
    "# Run the environment for one episode\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        action = model.predict(observation, deterministic=True)[0]\n",
    "    env.step(action)\n",
    "    frames.append(env.render())\n",
    "env.close()\n",
    "\n",
    "# Save the frames as a GIF\n",
    "imageio.mimsave('sample_episode.gif', frames, duration=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
